{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fasttrack setup\n",
    "\n",
    "Assuming that the yml-files are edited to correspond to your actual setup, then the following steps will create the SimpliVity-Connector together with Prometheus and Grafana environment. \n",
    "\n",
    "If you want to take a look into the details and go step by step using this Jupyter notebook, then jump to chapter 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f namespace.yml             # create the namespace\n",
    "kubectl apply -f svtdemo.yml               # apply the SimpliVity connector configmap (in the following example the name svtdemo was used)\n",
    "kubectl apply -f svtconnector.yml          # create the SimpliVity connector pod\n",
    "kubectl apply -f prometheus.configmap.yml  # create the Prometheus configmap\n",
    "kubectl apply -f prometheus.pv-claim.yml   # create the persistent volume for the Prometheus database\n",
    "kubectl apply -f prometheus.yml            # create the Prometheus pod\n",
    "kubectl apply -f grafana.pv-claim.yml      # create the persistent volume for the Grafana database\n",
    "kubectl apply -f grafana.yml               # create the grafana pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detailed Setup \n",
    "\n",
    "## 2.1 Create the namespace\n",
    "Create a namespace for your deployment. The example below creates the namespace svtprometheus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: svtprometheus\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create the SimpliVity - Prometheus connector POD\n",
    "\n",
    "### 2.2.1 Create the ConfigMap\n",
    "\n",
    "The connection and monitoring parameters are transferred to the HPE SimpliVity Prometheus connector as a Kubernetes ConfigMap. \n",
    "\n",
    "The ConfigMap can be created using th Python script: CreateConfigMap.py. \n",
    "\n",
    "Requirements:\n",
    "    - System with Python 3 and the following Python packages installed: \n",
    "      - Fernet\n",
    "      - getpass\n",
    "      - etree\n",
    "\n",
    "Run the script with the following command: \n",
    "\n",
    "#### python3 CreateConfigMap.py\n",
    "\n",
    "The script will ask for the following information:\n",
    "    - username               vCenter username (a user with readonly access rights is sufficient)\n",
    "    - password               vCenter password\n",
    "    - OVC/MVA IP address     IP address that the connector uses to connect to the federation\n",
    "    - name                   name of the yml-file (<name>.yml) and the configmap: <name>-xml that will be created\n",
    "    - port                   TCP Port that the connector uses to publish the counters.  \n",
    "\n",
    "It will create the Kubernetes yml-file (<name>.yml) that can be used to  create the Configmap <name>-xml \n",
    "\n",
    "The CreateConfigMap.py scripts sets for some of the SimpliVity connector parameters default values that can be edited in the file if needed:\n",
    "    \n",
    "    Parameter               Default value   Comment\n",
    "    - namespace             svtprometheus   adjust this to your namespace\n",
    "    - timerange             30              A range in seconds (the duration from the specified point in time)\n",
    "    - resolution            SECOND          The resolution (SECOND, MINUTE, HOUR, or DAY)\n",
    "    - monitoringinterval    30              connector cyle time (should be >= the time to process the captured data)\n",
    "    - monitor               fcn             performance data capture selector: f(ederation), c(luster), n(ode), v(irtual machine)\n",
    "    - cluster                               enter a clustername if you want to limit the data capture to a single cluster\n",
    "    - limit                 500             A positive integer that represents the maximum number of results to return\n",
    "    - offset                -1              A positive integer that directs the service to start returning the <offset value> instance, up to the limit. Every result will be collected if the offset is set to a negative value.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the configmap (in the following example the name svtdemo was used)\n",
    "kubectl apply -f svtdemo.yml"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example for the SimpliVity Prometheus Connector Configmap = do not use this example code.\n",
    "# Create your own configmap using the CreateConfigMap.py script! \n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: svtdemo-xml\n",
    "  namespace: test\n",
    "data:\n",
    "  svtconnector.key: |-\n",
    "    urAmRTcW4ZAYEu072heHhVy=\n",
    "  svtconnector.xml: |-\n",
    "    <data>\n",
    "      <username>SvtCollector@vsphere.local</username>\n",
    "      <user>gAAAAABevov1_SDs9BOwuE1qIaMMuK8r_cPWhW9g_2AJoimwWo-4=</user>\n",
    "      <password>gAAAAABevov1qE1aqFc1K0fP0ax3gbo4zGYNG_ANQ==</password>\n",
    "      <ovc>10.80.40.98</ovc>\n",
    "      <timerange>30</timerange>\n",
    "      <resolution>SECOND</resolution>\n",
    "      <monitoringintervall>30</monitoringintervall>\n",
    "      <logfile>svtdemo.log</logfile>\n",
    "      <port>9091</port>\n",
    "      <monitor>fcn</monitor>\n",
    "      <cluster></cluster>\n",
    "      <limit>100</limit>\n",
    "      <offset>-1</offset>\n",
    "    </data>  \n",
    "EOF  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Create the SimpliVity connector POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: svtconnector\n",
    "  namespace: svtprometheus      \n",
    "  labels:\n",
    "    app: svtconnector\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: svtconnector\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: svtconnector\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: svtconnector\n",
    "          image: tb1378/svtconk8s\n",
    "          command: [\"/usr/bin/python3\"]\n",
    "          args: [\"/opt/svt/svtpromconnector.py\"]\n",
    "          volumeMounts:\n",
    "            - name: svtconnectorxml\n",
    "              mountPath: /opt/svt/data\n",
    "      volumes:\n",
    "        - name: svtconnectorxml\n",
    "          configMap:\n",
    "            name: svtdemo-xml   # the correct name of the configmap needs to be added here. \n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: svtconnector-service\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  selector:\n",
    "    app: svtconnector\n",
    "  ports:\n",
    "    - port: 9091               # The Port of that the SimpliVity connector uses\n",
    "      targetPort: 9091\n",
    "      protocol: TCP\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.3. Deploy PrometheusÂ¶\n",
    "\n",
    "The Prometheus POD will be deployed with service accounts, a Config Map for the input parameters and a persistent volume for the Prometheus database. We will use here three yml-files (prometheus.configmap.yml, prometheus.pv-claim.yml and prometheus.yml) instead of a single one, in order to make the process a bit easier to understand.\n",
    "\n",
    "### 2.3.1 Create the Prometheus Configmap\n",
    "\n",
    "The Config Map will be used to define the Prometheus monitoring jobs. Each SimpliVity connector will require a separate Prometheus monitoring job, where each job will have the following entries:\n",
    "\n",
    "- job_name: 'simplivity-demo'\n",
    "  static_configs:\n",
    "  - targets: ['svtconnector:9091'] \n",
    "  honor_timestamps: true\n",
    "  scrape_interval: 30s\n",
    "  scrape_timeout: 10s\n",
    "  metrics_path: /metrics\n",
    "  scheme: htt\n",
    "The above job, will scrape every 30 seconds the data from the target svtconnector:9091, where svtconnector is the name and 9091 is the port that is used by the SimpliVity Prometheus connector container (as it was deployed in 3.) The target name, port and scrape_intervall should be adjusted to the actual values of the actual implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    " name: prometheus-config\n",
    " namespace: svtprometheus\n",
    "data:\n",
    " prometheus.yml: |\n",
    "  global:\n",
    "    scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n",
    "    evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n",
    "  alerting:\n",
    "    alertmanagers:\n",
    "    - static_configs:\n",
    "      - targets:\n",
    "        # - alertmanager:9093\n",
    "\n",
    "  # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\n",
    "  rule_files:\n",
    "    # - \"first_rules.yml\"\n",
    "    # - \"second_rules.yml\"\n",
    "\n",
    "  # A scrape configuration containing exactly one endpoint to scrape:\n",
    "  # Here it's Prometheus itself.\n",
    "  scrape_configs:\n",
    "    # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n",
    "    - job_name: 'prometheus'\n",
    "\n",
    "      # metrics_path defaults to '/metrics'\n",
    "      # scheme defaults to 'http'.\n",
    "\n",
    "      static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "      \n",
    "    - job_name: 'simplivity-demo'\n",
    "      static_configs:\n",
    "      - targets: ['svtdemo:9091']\n",
    "      honor_timestamps: true\n",
    "      scrape_interval: 30s\n",
    "      scrape_timeout: 10s\n",
    "      metrics_path: /metrics\n",
    "      scheme: http\n",
    "    - job_name: 'ctcinfrastructure'\n",
    "      static_configs:\n",
    "      - targets: ['svtinfra:9091']\n",
    "      honor_timestamps: true\n",
    "      scrape_interval: 30s\n",
    "      scrape_timeout: 10s\n",
    "      metrics_path: /metrics\n",
    "      scheme: http\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Create the persistent volume claim for the database\n",
    "\n",
    "A persistent volume claim is used for the Prometheus database content.\n",
    "The following example uses a NFS share for the persistent volume of the Prometheus database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: prom-db-volume\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  persistentVolumeReclaimPolicy: Recycle\n",
    "  volumeMode: Filesystem\n",
    "  storageClassName: \"nfs\"\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  nfs:\n",
    "    server: 10.1.41.12\n",
    "    path: /k8s/pvc/prometheus\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: prometheus-pv-claim\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  storageClassName: \"nfs\"\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Create the Prometheus POD and ServiceÂ¶\n",
    "The Prometheus POD is created with the Prometheus Config Map and the persistent volume claim, that were defined in step 2.3.1 and 2.3.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "rules:\n",
    "- apiGroups: [\"\"]\n",
    "  resources:\n",
    "  - nodes\n",
    "  - nodes/proxy\n",
    "  - services\n",
    "  - endpoints\n",
    "  - pods\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- apiGroups:\n",
    "  - extensions\n",
    "  resources:\n",
    "  - ingresses\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- nonResourceURLs: [\"/metrics\"]\n",
    "  verbs: [\"get\"]\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: prometheus\n",
    "roleRef:\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "  kind: ClusterRole\n",
    "  name: prometheus\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: grafana\n",
    "      tier: backend\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: grafana\n",
    "        tier: backend\n",
    "    spec:\n",
    "      serviceAccountName: prometheus\n",
    "      containers:\n",
    "        - name: prometheus\n",
    "          image: prom/prometheus\n",
    "          args:\n",
    "            - '--storage.tsdb.path=/prometheus'\n",
    "            - '--storage.tsdb.no-lockfile'\n",
    "            - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "          ports:\n",
    "            - containerPort: 9090\n",
    "              name: prometheus-port\n",
    "          volumeMounts:\n",
    "            - name: config-prometheus\n",
    "              mountPath: \"/etc/prometheus/\"\n",
    "            - name: prometheus-db\n",
    "              mountPath: \"/prometheus\"\n",
    "      volumes:     \n",
    "        - name: config-prometheus\n",
    "          configMap:\n",
    "            name: prometheus-config\n",
    "        - name: prometheus-db\n",
    "          persistentVolumeClaim:\n",
    "            claimName: prometheus-pv-claim\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "  labels: \n",
    "     hpecp.hpe.com/hpecp-internal-gateway: \"true\"\n",
    "spec:\n",
    "  selector:\n",
    "    app: grafana\n",
    "    tier: backend\n",
    "  ports:\n",
    "    - port: 9090\n",
    "      targetPort: 9090\n",
    "      protocol: TCP\n",
    "  type: LoadBalancer\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Deploy Grafana\n",
    "\n",
    "### 2.4.1 Create the persistent volume for the Grafana Database\n",
    "The following example shows a persistent volume provided by a NFS server (10.1.41.12) on the path /k8s/pvc/grafana.\n",
    "The actual persistent volume the will be used, need to be adjusted to reflect your environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: grafana-db-volume\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  persistentVolumeReclaimPolicy: Retain # Recycle, Delete\n",
    "  volumeMode: Filesystem\n",
    "  storageClassName: \"nfs\"\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  nfs:                        \n",
    "    server: 10.1.41.12        \n",
    "    path: /k8s/pvc/grafana\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: grafana-pv-claim\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  storageClassName: \"nfs\"\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Create the Grafana POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: grafana-service\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  selector:\n",
    "    app: grafana\n",
    "    tier: frontend\n",
    "  ports:\n",
    "    - port: 3000\n",
    "      targetPort: 3000\n",
    "      protocol: TCP\n",
    "  type: LoadBalancer\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: grafana\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: grafana\n",
    "      tier: frontend\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: grafana\n",
    "        tier: frontend\n",
    "    spec:     \n",
    "      containers:\n",
    "        - name: grafana\n",
    "          image: grafana/grafana\n",
    "          ports:\n",
    "            - containerPort: 3000\n",
    "              name: grafana-port\n",
    "              \n",
    "          volumeMounts:\n",
    "            - name: grafana-persistent-storage\n",
    "              mountPath: \"/var/lib/grafana\"\n",
    "          env:  \n",
    "            - name: GF_SMTP_ENABLED\n",
    "              value: \"yes\"\n",
    "            - name: GF_SMTP_HOST\n",
    "              value: \"smtp.hpe.com:25\"\n",
    "            - name: GF_SMTP_FROM_NAME\n",
    "              value: \"Beha, Thomas\"\n",
    "            - name: GF_SMTP_FROM_ADDRESS\n",
    "              value: \"thomas.beha@hpe.com\"\n",
    "            - name: GF_SMTP_SKIP_VERIFY\n",
    "              value: \"true\"\n",
    "            - name: GF_SERVER_HTTP_PORT\n",
    "              value: \"3000\"\n",
    "            - name: GF_INSTALL_PLUGINS\n",
    "              value: \"grafana-kubernetes-app\"\n",
    "#           - name: GF_INSTALL_PLUGINS\n",
    "#             value: \"ryantxu-ajax-panel\"\n",
    "#           - name: GF_PANELS_DISABLE_SANITIZE_HTML\n",
    "#             value: \"true\"\n",
    "#          - name: GF_AUTH_DISABLE_LOGIN_FORM\n",
    "#            value: \"true\"\n",
    "#          - name: GF_AUTH_ANONYMOUS_ENABLED\n",
    "#            value: 'true'\n",
    "      volumes:\n",
    "        - name: grafana-persistent-storage\n",
    "          persistentVolumeClaim:\n",
    "            claimName: grafana-pv-claim  \n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Check that everything is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl -n svtprometheus get pods,svc,configmaps,pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Delete everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f grafana.yml       \n",
    "kubectl delete -f grafana.pv-claim.yml\n",
    "kubectl delete -f prometheus.yml\n",
    "kubectl delete -f prometheus.pv-claim.yml\n",
    "kubectl delete -f prometheus.configmap.yml\n",
    "kubectl delete -f svtconnector.yml \n",
    "kubectl delete -f svtdemo.yml \n",
    "kubectl delete -f namespace.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "bash",
   "name": "python_defaultSpec_1595593951567"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}